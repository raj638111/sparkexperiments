

   #  When to use?
      +  When Schema is not known until runtime #1
      
   #  How?
      1. Create an RDD of Rows from the original RDD;
      2. Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
      3. Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext.
      
   #  Example
      // Create an RDD
      val people = sc.textFile("examples/src/main/resources/people.txt")
      
      // The schema is encoded in a string
      val schemaString = "name age"
      
      // Import Row.
      import org.apache.spark.sql.Row;
      
      // Import Spark SQL data types
      import org.apache.spark.sql.types.{StructType,StructField,StringType};
      
      // Generate the schema based on the string of schema
      val schema =
        StructType(
          schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
      
      // Convert records of the RDD (people) to Rows.
      val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
      
      // Apply the schema to the RDD.
      val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)
      
      // Register the DataFrames as a table.
      peopleDataFrame.registerTempTable("people")
      
      // SQL statements can be run by using the sql methods provided by sqlContext.
      val results = sqlContext.sql("SELECT name FROM people")
      
      // The results of SQL queries are DataFrames and support all the normal RDD operations.
      // The columns of a row in the result can be accessed by field index or by field name.
      results.map(t => "Name: " + t(0)).collect().foreach(println)   
               
      
   http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema #1      