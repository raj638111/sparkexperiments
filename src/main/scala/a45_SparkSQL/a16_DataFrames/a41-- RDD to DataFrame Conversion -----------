

   #  Benefits over RDDs  
      +  Enables spark to perform filtering & more with deserialization #1
   
   #  Example
      // Encoders for most common types are automatically provided by importing sqlContext.implicits._
      val ds = Seq(1, 2, 3).toDS()
      ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)
      
      // Encoders are also created for case classes.
      case class Person(name: String, age: Long)
      val ds = Seq(Person("Andy", 32)).toDS()
      
      // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.
      val path = "examples/src/main/resources/people.json"
      val people = sqlContext.read.json(path).as[Person]   
      
   http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets #1
   http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder  #2      
      